<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Cost Maps for HAN with Graph Neural Networks | Daniel Rodriguez-Criado</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <link rel="icon" type="image/png" href="../../img/favicon/rocket-solid.png">
    <script src="https://kit.fontawesome.com/55d924b99d.js" crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,300;0,500;0,700;0,900;1,300&amp;display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Anton&amp;display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../css/projects/projectStyles.css">
  </head>
  <body>
    <div class="global-container">
      <input type="checkbox" autocomplete="off" id="menu-icon">
      <label for="menu-icon"><i class="fas fa-bars"></i></label>
      <header class="main-header">
        <h1 class="main-header__title">Cost Maps for HAN with Graph Neural Networks</h1>
      </header>
      <aside class="main-aside">
        <h1 class="main-aside__title"><a href="../../html/pages/research.html">Research projects</a></h1>
        <div class="main-aside__nav-filter">
          <input type="text" id="filter" placeholder="Search for projects...">
        </div>
        <nav class="main-nav">
          <ul class="main-menu" id="menu">
            <li class="main-menu__item"><a href="traffic.html">Generation of traffic images</a>
              <ul class="main-menu__submenu">
                <li class="submenu__item"><a href="#introduction">Introduction</a></li>
                <li class="submenu__item"><a href="#results">Results</a></li>
                <li class="submenu__item"><a href="#citation">Citation</a></li>
              </ul>
            </li>
            <li class="main-menu__item"><a href="sonata.html">SONATA</a>
              <ul class="main-menu__submenu">
                <li class="submenu__item"><a href="#introduction">Introduction</a></li>
                <li class="submenu__item"><a href="#requirements">Software Requirements</a></li>
                <li class="submenu__item"><a href="#usage">How to run the default tool</a></li>
                <li class="submenu__item"><a href="#tutorial">Tutorial: Customize Sonata</a></li>
              </ul>
            </li>
            <li class="main-menu__item"><a href="sngnn1d.html">SNGNN1D</a>
              <ul class="main-menu__submenu">
                <li class="submenu__item"><a href="#motivation">Motivation</a></li>
                <li class="submenu__item"><a href="#scenes">Software Requirements</a></li>
                <li class="submenu__item"><a href="#results">How to run the default tool</a></li>
                <li class="submenu__item"><a href="#references">References</a></li>
              </ul>
            </li>
            <li class="main-menu__item"><a href="sngnn1dv2.html">SNGNN1D-V2</a>
              <ul class="main-menu__submenu">
                <li class="submenu__item"><a href="#motivation">Motivation</a></li>
              </ul>
            </li>
            <li class="main-menu__item--active"><a href="sngnn2d.html">SNGNN2D</a>
              <ul class="main-menu__submenu">
                <li class="submenu__item"><a href="#motivation">Motivation and aims</a></li>
                <li class="submenu__item"><a href="#howtest">How to try our model</a></li>
                <li class="submenu__item"><a href="#dataset">Dataset generation</a></li>
                <li class="submenu__item"><a href="#test">Testing the cost map with an A* planner</a></li>
              </ul>
            </li>
            <li class="main-menu__item"><a href="datasets.html">Datasets</a>
              <ul class="main-menu__submenu">
                <li class="submenu__item"><a href="#socnav1">SocNav1</a></li>
                <li class="submenu__item"><a href="#socnav2">SocNav2</a></li>
              </ul>
            </li>
          </ul>
        </nav>
      </aside>
      <main class="main-content">
        <h1>Cost map for social navigation generated with a GNN</h1>
        <p>The project <b>SNGNN2D</b> pursuits to generate a 2D cost map that can be used for social navigation. The model is trained with a dataset of 2D images bootstrapped from 1D features. For doing that, we have made use of the model developed in SNGNN1D project as detailed in the <a href="#test">dataset section</a>. Additionally, we have used an A* planner to evaluate the efficiency and social compliance of the cost map (see <a href="#test">section</a>).</p>
        <video width="800" height="400" control>
          <source src="../../videos/projects/sngnn2d/ICRAvideo.mp4" type="video/webm">Your browser does not support the video tag.
        </video><a href="https://github.com/gnns4hri/graph2image">GitHub repository</a>
        <section class="main-section">
          <h2 class="main-section__title"><a name="motivation">Motivation and aims</a></h2>
          <div class="main-section__content"></div>
          <p>This project aims to provide a model for robot disruption in human comfortability that can efficiently generate two-dimensional cost maps for HAN considering interactions, an area that has been overlooked until recently. The <b>contributions</b> of the project are two-fold: <b>a)</b> a technique to bootstrap two-dimensional datasets from one-dimensional datasets; and <b>b)</b> <b>SNGNN-2D</b>, an architecture that combines Graph Neural Networks (GNN) and Convolutional Neural Networks (CNN) to generate two dimensional cost maps based on the robot's knowledge.</p>
          <p>After training, the resulting ML architecture is able to efficiently generate cost maps that can be used as a cost function for Human-Aware Navigation.</p>
          <figure><img src="../../img/projects/sngnn2d/model.png" alt="table with GMM results" style="width: 500px">
            <figcaption><b>Fig. 1:</b> Model pipeline.</figcaption>
          </figure>
        </section>
        <section class="main-section">
          <h2 class="main-section__title"><a name="howtest">How to try our model</a></h2>
          <div class="main-section__content">
            <h2>Using Docker</h2>
            <h2>Regular instalation</h2>
          </div>
        </section>
        <section class="main-section">
          <h2 class="main-section__title"><a name="dataset">Dataset generation</a></h2>
          <div class="main-section__content">
            <p>The acquisition of two-dimensional cost or disruption maps to create datasets for learning purposes generates a number of challenges. It also requires a significant commitment in comparison to their scalar value counterparts. A factor to consider is that the precision of the answer is dependent on the subjects' capability to represent the situation and their preferences graphically. Their inclination and motivation to stay engaged in the task is an additional challenge.</p>
            <p>From an ML perspective, when factoring in an approximately equal time commitment and effort when generating answers, providing a single scalar for each scenario would yield answers for a higher number of scenarios. This would in turn generate a higher variability in the input scenarios that would make the model less prone to overfitting.</p>
            <figure><img src="../../img/projects/sngnn2d/dataset1.png" alt="screenshot of the SocNav1 tool" style="width: 640px">
              <figcaption><b>Fig. 2:</b> SNGNN-1D  can  be  used  to  estimate  the  disruption  caused  by the robot given a particular scenario.</figcaption>
            </figure>
            <figure><img src="../../img/projects/sngnn2d/dataset2.png" alt="screenshot of the SocNav1 tool" style="width: 640px">
              <figcaption><b>Fig. 3:</b> The  expected  2D  outputs  are  generated  performing  multiplequeries to SNGNN-1D, shifting the scenario around the robot.</figcaption>
            </figure>
            <p>A dataset containing scalars as output data cannot directly be used to train a model which provides two dimensional output, so the approach followed in this case is to use a model which provides one-dimensional value estimations (SNGNN-1D) and sample its output shifting the robot's position, bootstrapping this way a two-dimensional dataset. The process of sampling is depicted in Fig.1 and 2. For each scenario in the bootstrapped dataset a matrix of 73x73 samples is generated. A total of 37131 scenarios were randomly generated following the same strategy of SocNav1. The dataset split for training, development and test is of 31191, 2970 and 2970 scenarios, respectively.</p>
          </div>
        </section>
        <section class="main-section">
          <h2 class="main-section__title"><a name="test">Testing the cost map with an A* planner</a></h2>
          <div class="main-section__content">
            <p>To assess the effectiveness of SNGNN-2D, this section presents simulated navigation results and a comparison with the social aware navigation approach proposed in~\cite{Vega2019}, which is based in Gaussian Mixture Models (GMMs).</p>
            <p>The experiments were conducted under simulated environments using <a href="sonata.html">SONATA</a>, a toolkit built on top of PyRep designed to simulate human-populated navigation scenarios and to generate datasets. <a href="sonata.html">SONATA</a> provides an API to generate random scenarios including humans, objects, interactions, the robot and its goals. The walls delimiting a room are also randomly generated considering rectangular and L-shaped rooms.</p>
            <p>SONATA also provides real-time access to the information of the elements in the environment and their properties. This information is used by the two tested methods to generate a cost map, which is integrated in a control system in charge of planning a minimum cost path (using A*) and moving the robot towards the goal position.</p>
            <p>According to the number of humans in the room, three different types of scenarios were tested: rooms with 2 standing humans and 1 walking human \(S_A\), rooms with 4 standing humans and 2 walking humans \(S_B\) and rooms with 5 standing humans and 3 walking humans \(S_C\). All the scenarios included a randomly generated number of objects, room shape and wall length.  The number of interactions between humans or humans and objects was also randomly generated.  For each type of scenario, each method was executed in 50 different simulations to cover a wide range of situations. The results of applying each method were evaluated according to the following metrics:</p>
            <ul>
              <li>\(\tau\): navigation time</li>
              <li>\(d_t\): travelled distance</li>
              <li>\(CHC\): cumulative heading changes</li>
              <li>\(d_{min}\): minimum distance to a human</li>
              <li>\(si_i\): number of intrusions into the intimate space of humans (closer than 0.45m</li>
              <li>\(si_p\): number of intrusions into the personal space of humans (closer than 1.2m</li>
              <li>\(si_r\): number of intrusions into an interaction (closer than 0.5m</li>
            </ul>
            <p>The following tables show the mean and the standard deviation of these metrics using SNGNN-2D and the GMM-based method, respectively, considering separately each group of scenarios. For the first two types of scenarios (\(S_A\) and \(S_B\)) results in relation to the mean values of most of the metrics can be considered comparable, although SNGNN-2D produces better results according to the travelled distance \(d_t\) and the cumulative heading changes \(CHC\). More variability is observed in the GMM-based approach as can be observed by the standard deviation of each parameter. In addition, for complex scenarios \(S_C\) greater differences can be observed between the two methods, showing that the proposed model behaves in a more socially acceptable way in crowded environments.</p>
            <figure><img src="../../img/projects/sngnn2d/ourResults.png" alt="table with our results" style="width: 500px">
              <figcaption><b>TABLE 1:</b> Navigation results using the GMM-based method. Mean and Standard Deviation of the metrics for each group of scenarios.</figcaption>
            </figure>
            <figure><img src="../../img/projects/sngnn2d/GMMResults.png" alt="table with GMM results" style="width: 500px">
              <figcaption><b>TABLE 2:</b> Navigation results using SNGNN-2D. Mean and Standard Deviation of the metrics for each group of scenarios.</figcaption>
            </figure>
          </div>
        </section>
      </main>
    </div>
  </body>
  <script src="../../js/indexProjects.js"></script>
</html>