<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Daniel Rodriguez Criado - Research projects</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <!-- <link rel="icon" type="image/png" href="../img/favicon/rocket-solid.png"> -->
    <script src="https://kit.fontawesome.com/55d924b99d.js" crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;1,300;1,400&amp;display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../css/projects/projectStyles.css">
  </head>
  <body>
    <div class="global-container">
      <input type="checkbox" id="menu-icon">
      <label for="menu-icon"><i class="fas fa-bars"></i></label>
      <header class="main-header">
        <a href="../pages/research.html"><h1 class="main-header__title">My research projects page</h1></a>
      </header>
      <aside class="main-aside">
        <h1 class="main-aside__title"><a href="../pages/research.html">Research projects</a></h1>
        <div class="main-aside__nav-filter">
          <input type="text" id="filter" onkeyup="myFilter()" placeholder="Search for projects...">
        </div>
        <nav class="main-nav">
          <ul class="main-menu" id="menu">
            <li class="main-menu__item"><a href="traffic.html">Generation of traffic images</a></li>
            <li class="main-menu__item"><a href="sonata.html">Sonata</a></li>
            <li class="main-menu__item--active"><a href="#">SNGNN1D</a>
              <ul class="main-menu__submenu">
                <li class="submenu__item"><a href="#motivation">Motivation</a></li>
                <li class="submenu__item"><a href="#scenes">What are the scenes composed of?</a></li>
                <li class="submenu__item"><a href="#results">Results</a></li>
                <li class="submenu__item"><a href="#references">References</a></li>
              </ul>
            </li>
            <li class="main-menu__item"><a href="sngnn1dv2.html">SNGNN1D-V2</a></li>
            <li class="main-menu__item"><a href="sngnn2d.html">SNGNN2D</a></li>
            <li class="main-menu__item"><a href="datasets.html">Datasets</a></li>
         </ul>
        </nav>
      </aside>
      <main class="main-content">


        <p><b>SNGNN</b> is a Graph Neural Network to model adherence to social-navigation conventions for robots. Given a particular scenario composed of a room with any number of walls, objects and people (who can be interacting with each other) the network provides a social adherence ratio from 0 to 1. This information can be used to plan paths for human-aware navigation.
        <p>
        <br/>

        <a href="https://github.com/robocomp/sngnn">GitHub repository</a>.

      
      
        <section class="main-section">
          <h2 class="main-section__title"><a name="motivation">Motivation</a></h2>
          <div class="main-section__content">
            <p>
            As most researchers working on human-aware navigation, we used to handcraft the proxemics models our robots used for navigation.
            For instance, in our paper <em>"Socially Acceptable Robot Navigation over Groups of People"</em> (<a href="https://ieeexplore.ieee.org/document/8172454">link</a>) we used Gaussian Mixture Models to generate estimations of how irritating the presence of robots is in the different locations of any given environment (see <b>Fig. 1</b>).
            <figure>
            <img src="../img/projects/sngnn1d/GMMs.png" alt="gausian mixture models used in 2017" style="width: 400px;" />
            <figcaption>
            <b>Fig. 1:</b> Example of the use of Gaussian Mixture Models to model proxemics (2017).
            </figcaption>
            </figure>
            <p>

            <p>
            It worked quite well, but it had limitations regarding scalability with respect to the number of factors to consider.
            The models becoming slower was not the biggest of our problems.
            The complexity of the code, the number of bugs to deal with and the time necessary to develop these new features made the process hard and expensive.
            At some point we realised that following a (hybrid) data-driven approach would probably be a good idea, especially more cost-efficient than hand-engineering the models.
            Additionally, it would allow us to investigate into aspects which we did not consider because we were aware of their importance.
            </p>
          </div>
        </section>


        

        <section class="main-section">
          <h2 class="main-section__title"><a name="scenes">What are the scenes composed of?</a></h2>
          <div class="main-section__content">
            <p>To choose the best ML model we have first to consider the nature of the data. Which are the main characteristics of the data usually considered in human-aware navigation? 
            <ul>
            <li>Heterogeneous: many factors</li>
            <li>Variable number of people</li>
            <li>Variable environment (variable shaped and sized)</li>
            <li>Variable number of objects</li>
            <li>Variable number of interactions</li>
            <li>Variable number of <b>types of</b> interactions</li>
            <li>Indeterminately complex &amp; structured relationships</li>
            </ul>
            </p>

            <p>
            Considering all this variability in the input data, and especially its size and highly structured nature, it would be quite difficult to handcraft good descriptors for the scenarios that could be used for regular fully-connected NNs.
            Convolutional Neural Networks or conventional Recurrent Networks did not seem to be a good match for the data either.
            Therefore we decided to use Graph Neural Networks (GNNs, <a href="https://arxiv.org/abs/1806.01261">reference</a>).
            </p>

            <p>
            Using GNNs for human-aware navigation allows us to improve accuracy of other ML algorithms (see references at the bottom) and improve scalability (how can we increase the number of variables to consider?) for different tasks.
            Some of these tasks are:
            <ul>
            <li>Model proxemics / inconvenience</li>
            <li>Predict people's movements</li>
            <li>Control robot's movements</li>
            <li>Detect &amp; predict behaviours/events</li>
            </ul>
            </p>

            <p>
            We were able to obtain labels from 0 to 100 for 9280 randomly generated scenarios comprising scenarios with varying data as described in the previous paragraphs.
            The tool used to generate the data is shown in <b>Fig. 2</b>.
            Even though the results we obtained are good, we are aware of some limitations that will be addressed in future datasets: a) humans are static, b) there is only one type of interaction, c) we are told <em>"how people <b>think</b> they <b>would</b> feel"</em>, not how they actually felt in the situation.
            </p>
            <p>
            The mean squared error (<b>MSE</b>) achieved for the dataset is <b>0.03173</b>. Humans' MSE is 0.02929.
            </p>

            <figure>
            <img src="../img/projects/sngnn1d/scenario_d.jpg" alt="screenshot of the SocNav1 tool" style="width: 640px" />
            <figcaption>
            <b>Fig. 2:</b> Example of the use of Gaussian Mixture Models to model proxemics (2017).
            </figcaption>
            </figure>
          </div>
        </section>

        
        
        
       
        
        
        
        
        <section class="main-section">
          <h2 class="main-section__title"><a name="results">Results</a></h2>
          <div class="main-section__content">
            <p>The following videos demonstrate the results obtained and showcase some of the properties of SNGNN.</p>

            <br/>
            <br/>
            <h3><b>Distance between two interacting people</b></h3>
            <p>
            In this video you can see how the distance between two <b>interacting</b> people affects the acceptance of the presence of a robot.
            </p>
            <video width="800" height="400" controls>
                <source src="../videos/projects/sngnn1d/showcase_7_1.webm" type="video/webm" /> Your browser does not support the video tag.
            </video>

            <br/>
            <br/>
            <h3><b>Impact of walls</b></h3>
            <p>
            In this video you can see how the distance between a wall and a person affects the acceptance of the presence of a robot.
            Surprisingly, the difference is not very noticeable but it is in line with existing studies.
            </p>
            <video width="800" height="400" controls>
                <source src="../videos/projects/sngnn1d/showcase_2_1.webm" type="video/webm" /> Your browser does not support the video tag.
            </video>

            <br/>
            <br/>
            <h3><b>Impact of incrementing the number of people in a room</b></h3>
            <p>
            This video showcases the ability of the network to adapt to an environment with a variable number of people.
            The response is as it would be expected: their spaces shrink as the density of people in the room increases.
            For example, people are way more relaxed about personal spaces in lifts than in open spaces.
            </p>
            <video width="800" height="400" controls>
                <source src="../videos/projects/sngnn1d/showcase_3_3.webm" type="video/webm" /> Your browser does not support the video tag.
            </video>

            <br/>
            <br/>
            <h3><b>Angle when approaching interacting people</b></h3>
            <p>
            We can appreciate in this video that the network is able to tell that, if a robot has to cross two people who are interacting, it should do it perpendicular to the line of interaction (i.e., the value of the function is minimum when the angle is perpendicular).
            </p>
            <video width="800" height="400" controls>
                <source src="../videos/projects/sngnn1d/showcase_5_1.webm" type="video/webm" /> Your browser does not support the video tag.
            </video>

            <br/>
            <br/>
            <h3><b>Stability of the results</b></h3>
            <p>
            This video shows a simulated environment where the robot is being <b>MANUALLY MOVED WITH A JOYSTICK</b>. Its purpose is to show the response of the network for the different positions and its stability.
            </p>
            <video width="800" height="400" controls>
                <source src="../videos/projects/sngnn1d/sngnn.webm" type="video/webm" /> Your browser does not support the video tag.
            </video>
          </div>
        </section>
        
        
        
        <section class="main-section">
          <h2 class="main-section__title"><a name="references">References</a></h2>
          <div class="main-section__content">
            <p style="text-align: left"><b>Social-Navigation Graph Neural Network:</b></p>
            <ul>
                <li><b>Graph Neural Networks for Human-aware Social Navigation</b>. <em>L.J. Manso, R.R. Jorvekar, D.R. Faria, P. Bustos, P. Bachiller</em>. arXiv preprint arXiv:1909.09003. 2019.</li>
                <li><b>SocNav1: A Dataset to Benchmark and Learn Social Navigation Conventions</b>. <em>L.J. Manso, P. Nunez, L.V. Calderita, D.R. Faria, P. Bachiller</em>. arXiv preprint arXiv:1909.02993. 2019.</li>
                <li><b>Learning Human-Object Interactions by Graph Parsing Neural Networks</b>.<em>S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu</em>. arXiv:1808.07962. 2018.</li>
                <li><b>Relational Graph Learning for Crowd Navigation</b>. <em>C. Chen, S. Hu, P. Nikdel, G. Mori, M. Savva</em>. arXiv preprint arXiv:1909.13165. 2019.</li>
            </ul>
            <p style="text-align: left"><b>Graph Neural Networks:</b></p>
            <ul>
                <li><b>Semi-Supervised Classification with Graph Convolutional Networks</b>. <em>T.N. Kipf, M. Welling</em>. arXiv preprint arXiv:1609.02907. 2017.</li>
                <li><b>Relational inductive biases, deep learning, and graph networks</b>. <em>P.W. Battaglia, J.B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre, F. Song, A. Ballard, J. Gilmer, G. Dahl, A. Vaswani, K. Allen, C. Nash, V. Langston, C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, R. Pascanu</em>. arXiv preprint arXiv:1806.01261. 2018.</li>
            </ul>
            <p style="text-align: left !important"><b>Slides: </b> &nbsp; <a href="https://ljmanso.com/perspectives/">https://ljmanso.com/perspectives/</a></p>
        </div>
        </section>

        
      </main>
    </div>
  </body>
  <script src="../js/projects/index.js"></script>
</html>



































































